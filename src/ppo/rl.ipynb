{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cda3c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2258ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping from piece symbols to plane indices\n",
    "# White pieces are in planes 0-5, Black pieces are in planes 6-11\n",
    "piece_to_plane = {\n",
    "    ('P', chess.WHITE): 0,\n",
    "    ('N', chess.WHITE): 1,\n",
    "    ('B', chess.WHITE): 2,\n",
    "    ('R', chess.WHITE): 3,\n",
    "    ('Q', chess.WHITE): 4,\n",
    "    ('K', chess.WHITE): 5,\n",
    "    ('p', chess.BLACK): 6,\n",
    "    ('n', chess.BLACK): 7,\n",
    "    ('b', chess.BLACK): 8,\n",
    "    ('r', chess.BLACK): 9,\n",
    "    ('q', chess.BLACK): 10,\n",
    "    ('k', chess.BLACK): 11,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4ab9980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def board_to_tensor(board: chess.Board) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert a python-chess board object to a (17, 8, 8) PyTorch Tensor\n",
    "    The 17 planes:\n",
    "    - 0-5: White pieces (P, N, B, R, Q, K)\n",
    "    - 6-11: Black pieces (p, n, b, r, q, k)\n",
    "    - 12: Player to move (1 for white, 0 for black)\n",
    "    - 13: White's kingside castling right\n",
    "    - 14: White's queenside castling right\n",
    "    - 15: Black's kingside castling right\n",
    "    - 16: Black's queenside castling right\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a 17x8x8 NumPy array with zeros\n",
    "    tensor_np = np.zeros((17, 8, 8), dtype=np.float32)\n",
    "\n",
    "    # Populate piece planes (0 - 11)\n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        if piece:\n",
    "            # Get the plane index for the piece type and color\n",
    "            plane_idx = piece_to_plane[(piece.symbol(), piece.color)]\n",
    "\n",
    "            # The board is read from A1 to H8, but we want a standard matrix view\n",
    "            # rank (row) 7 -> 0, 6 -> 1, ..., 0 -> 7\n",
    "            # file (col) 0 -> 0, 1 -> 1, ..., 7 -> 7\n",
    "            rank = chess.square_rank(square)\n",
    "            file = chess.square_file(square)\n",
    "            tensor_np[plane_idx, 7-rank, file] = 1\n",
    "    \n",
    "    # Populate state planes (12 - 16)\n",
    "    # Plane 12: PLayer to move\n",
    "    if board.turn == chess.WHITE:\n",
    "        tensor_np[12, :, :] = 1\n",
    "    else:\n",
    "        tensor_np[12, :, :] = 0 # Not necessary due to np.zeros, but for clarity\n",
    "\n",
    "    # Plane 13 - 16: Castling rights\n",
    "    if board.has_kingside_castling_rights(chess.WHITE):\n",
    "        tensor_np[13, :, :] = 1\n",
    "    if board.has_queenside_castling_rights(chess.WHITE):\n",
    "        tensor_np[14, :, :] = 1\n",
    "    if board.has_kingside_castling_rights(chess.BLACK):\n",
    "        tensor_np[15, :, :] = 1\n",
    "    if board.has_queenside_castling_rights(chess.BLACK):\n",
    "        tensor_np[16, :, :] = 1\n",
    "\n",
    "    return torch.from_numpy(tensor_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac225fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the tensor: torch.Size([17, 8, 8])\n",
      "\n",
      "White pawns plane (should have 1s on the 2nd rank from bottom):\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "Player to move plane (should be all 1s for white):\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# --- Example usage ---\n",
    "\n",
    "# Create the board object \n",
    "board = chess.Board()\n",
    "\n",
    "# Convert the board to tensor\n",
    "board_tensor = board_to_tensor(board)\n",
    "\n",
    "# Print the shape\n",
    "print(f\"Shape of the tensor: {board_tensor.shape}\")\n",
    "\n",
    "# Let's check a few planes to see if it's correct\n",
    "print(\"\\nWhite pawns plane (should have 1s on the 2nd rank from bottom):\")\n",
    "print(board_tensor[0])\n",
    "\n",
    "print(\"\\nPlayer to move plane (should be all 1s for white):\")\n",
    "print(board_tensor[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5362ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for our chess data.\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_file, limit = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with FENs and evaluations.\n",
    "        \"\"\"\n",
    "        self.dataframe = pd.read_csv(csv_file)\n",
    "        if limit:\n",
    "            # Slice the dataframe to the specified limit\n",
    "            self.dataframe = self.dataframe.head(limit)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the FEN string and evaluation from the dataframe\n",
    "        fen = self.dataframe.iloc[idx]['FEN']\n",
    "        evaluation = self.dataframe.iloc[idx]['Evaluation'].lstrip('#')\n",
    "\n",
    "\n",
    "        # Create a board object\n",
    "        board = chess.Board(fen)\n",
    "        \n",
    "        # Convert the board to our tensor representation\n",
    "        board_tensor = board_to_tensor(board)\n",
    "        \n",
    "\n",
    "        # --- Normalize the evaluation score ---\n",
    "        # The raw score is in centipawns. Let's clamp it to a reasonable range,\n",
    "        # for example -1000 to +1000, which is like a +/- 10 pawn advantage.\n",
    "        # Then, we can scale it to the [-1, 1] range.\n",
    "        score = float(evaluation)\n",
    "        score_clamped = max(min(score, 1000), -1000)\n",
    "        # A simple scaling to [-1, 1]\n",
    "        normalized_score = score_clamped / 1000.0\n",
    "        \n",
    "        # Convert score to a tensor\n",
    "        eval_tensor = torch.tensor([normalized_score], dtype=torch.float32)\n",
    "\n",
    "        return board_tensor, eval_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1fa8236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 1000000 positions from ../dataset/ChessData.csv\n",
      "\n",
      "Testing the DataLoader with a batch size of 64...\n",
      "Batch 1:\n",
      "  Boards tensor shape: torch.Size([64, 17, 8, 8])\n",
      "  Evals tensor shape: torch.Size([64, 1])\n",
      "Batch 2:\n",
      "  Boards tensor shape: torch.Size([64, 17, 8, 8])\n",
      "  Evals tensor shape: torch.Size([64, 1])\n",
      "Batch 3:\n",
      "  Boards tensor shape: torch.Size([64, 17, 8, 8])\n",
      "  Evals tensor shape: torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "\n",
    "# 1. Create an instance of our dataset\n",
    "# Make sure you have 'chessData.csv' (or a sample of it) in the same directory\n",
    "try:\n",
    "    # Let's use the main dataset file\n",
    "    csv_path = '../dataset/ChessData.csv'\n",
    "    chess_dataset = ChessDataset(csv_file=csv_path, limit = 1000000)\n",
    "    print(f\"Successfully loaded {len(chess_dataset)} positions from {csv_path}\")\n",
    "\n",
    "    # 2. Create a DataLoader\n",
    "    # This will handle batching, shuffling, and can even use multiple CPU cores\n",
    "    batch_size = 64\n",
    "    data_loader = DataLoader(chess_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    # 3. Iterate over a few batches to see it in action\n",
    "    print(f\"\\nTesting the DataLoader with a batch size of {batch_size}...\")\n",
    "    for i, (boards, evals) in enumerate(data_loader):\n",
    "        print(f\"Batch {i+1}:\")\n",
    "        print(\"  Boards tensor shape:\", boards.shape) # Should be (batch_size, 17, 8, 8)\n",
    "        print(\"  Evals tensor shape:\", evals.shape)   # Should be (batch_size, 1)\n",
    "        \n",
    "        # We only need to check a few batches\n",
    "        if i == 2:\n",
    "            break\n",
    "            \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Make sure 'ChessData.csv' is in the same directory as the script.\")\n",
    "    print(\"You can download it from the Kaggle link you provided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03e391ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    An Actor-Critic network for our chess bot.\n",
    "    The network shares a convolutional body and has two heads:\n",
    "    1. Policy Head (Actor): Outputs move probabilities.\n",
    "    2. Value Head (Critic): Outputs a scalar value for the position\n",
    "    \"\"\"\n",
    "    def __init__(self, num_actions=4672):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # --- Shared Convolutional Body ---\n",
    "        # Input shape: (batch_size, 17, 8, 8)\n",
    "        self.conv1 = nn.Conv2d(17, 128, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "\n",
    "        # The output of the conv layers will be (batch_size, 256, 8, 8)\n",
    "        self.linear_input_size = 256 * 8 * 8\n",
    "\n",
    "        # --- Critic Head ---\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(self.linear_input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Tanh() # Outputs a value between -1 and 1\n",
    "        )\n",
    "\n",
    "        # --- Actor Head ---\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(self.linear_input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_actions) # Outputs logits for each possible action\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through the shared body\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "\n",
    "        # Flatten the output for the linear layers\n",
    "        x = x.view(-1, self.linear_input_size)\n",
    "\n",
    "        # Calcualte value and policy\n",
    "        value = self.value_head(x)\n",
    "        policy_logits = self.policy_head(x)\n",
    "\n",
    "        return policy_logits, value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14f78d",
   "metadata": {},
   "source": [
    "1. The Shared Convolutional Body (The \"Eyes\" of the Bot)\n",
    "\n",
    "Why Convolutional Layers (Conv2d)?\n",
    "Your input tensor (17, 8, 8) is structured like a multi-layered image. Convolutional Neural Networks (CNNs) are the industry standard for image recognition because they are brilliant at finding spatial patterns.\n",
    "\n",
    "    In chess, these \"patterns\" are things like pawn structures, open files, king safety, knight outposts, and tactical motifs.\n",
    "\n",
    "    A Conv2d layer slides a small filter (the kernel) across the 8x8 board to detect these patterns. A kernel_size=3 means it looks at a 3x3 square at a time.\n",
    "\n",
    "    Using padding=1 with a kernel_size=3 is a common trick to ensure the output of the layer is the same size as the input (8x8), preventing the board representation from shrinking.\n",
    "\n",
    "Why Multiple Layers?\n",
    "The network builds a hierarchy of knowledge:\n",
    "\n",
    "    conv1 (First Layer): Learns very simple patterns, like \"a white pawn is on this square\" or \"a bishop is attacking this square.\"\n",
    "\n",
    "    conv2 and conv3 (Deeper Layers): Combine the simple patterns from earlier layers into more complex and abstract concepts, like \"this is a fianchetto structure\" or \"the king is exposed.\" Increasing the number of channels (from 17 to 128, then to 256) allows the network to learn more of these patterns at each level.\n",
    "\n",
    "Why Batch Normalization (BatchNorm2d) and ReLU?\n",
    "\n",
    "    BatchNorm2d: This is a crucial regularization technique that helps make training faster and more stable. It normalizes the activations from the previous layer, preventing the signals from becoming too large or too small, which can stall the learning process.\n",
    "\n",
    "    ReLU (F.relu): This is the activation function. It introduces non-linearity, allowing the network to learn complex relationships. It's essentially the \"on/off switch\" for the detected patterns.\n",
    "\n",
    "2. The Critic Head (The \"Coach\")\n",
    "\n",
    "After the convolutional body has extracted all the important features into a high-level representation, this information is flattened into a long 1D vector (self.linear_input_size).\n",
    "\n",
    "Why Linear Layers?\n",
    "The nn.Linear layers act as the final decision-making part. They take the collection of identified patterns and learn to weigh their importance to calculate a final score.\n",
    "\n",
    "Why output a single number and use Tanh?\n",
    "\n",
    "    The goal of the Critic is to answer one question: \"How good is this position?\" The answer is a single number.\n",
    "\n",
    "    The nn.Tanh activation function is the key here. It squashes the output into a strict range between -1 and 1. This is critical because we normalized our training data (the Stockfish evaluations) to be in this exact same range. This makes the training objective very clear: make the network's output match the normalized expert score.\n",
    "\n",
    "3. The Actor Head (The \"Player\")\n",
    "\n",
    "The Actor head also receives the same flattened vector of features from the shared body.\n",
    "\n",
    "Why output num_actions values?\n",
    "\n",
    "    The Actor's job is to choose a move. It does this by assigning a score (a logit) to every single possible move in our predefined list (we used num_actions=4672). A higher logit means the network thinks that move is better.\n",
    "\n",
    "    We don't use an activation function like softmax here because the loss function we'll use for training the policy (Cross-Entropy Loss) prefers to work directly with the raw logits for better numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e75b63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully:\n",
      "ActorCritic(\n",
      "  (conv1): Conv2d(17, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (value_head): Sequential(\n",
      "    (0): Linear(in_features=16384, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (3): Tanh()\n",
      "  )\n",
      "  (policy_head): Sequential(\n",
      "    (0): Linear(in_features=16384, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=4672, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "--- Testing with a dummy batch ---\n",
      "Input shape: torch.Size([4, 17, 8, 8])\n",
      "Output policy logits shape: torch.Size([4, 4672])\n",
      "Output value estimate shape: torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "# Let's create an instance of our network\n",
    "model = ActorCritic()\n",
    "print(\"Model created successfully:\")\n",
    "print(model)\n",
    "\n",
    "# Now let's test it with a dummy batch of data from our DataLoader\n",
    "# This simulates one step of training\n",
    "dummy_batch_size = 4\n",
    "dummy_board_tensor = torch.randn(dummy_batch_size, 17, 8, 8)\n",
    "\n",
    "# Get the model's output\n",
    "policy_logits, value_estimate = model(dummy_board_tensor)\n",
    "\n",
    "print(\"\\n--- Testing with a dummy batch ---\")\n",
    "print(f\"Input shape: {dummy_board_tensor.shape}\")\n",
    "print(f\"Output policy logits shape: {policy_logits.shape}\") # Should be (batch_size, 4672)\n",
    "print(f\"Output value estimate shape: {value_estimate.shape}\")   # Should be (batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f245392f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "869fd142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Critic Pre-training ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Set the model to training mode\u001b[39;00m\n\u001b[32m     22\u001b[39m model.train()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (board_tensors, true_evals) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mdata_loader\u001b[49m):\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# Move tensors to the correct device\u001b[39;00m\n\u001b[32m     26\u001b[39m     board_tensors = board_tensors.to(device)\n\u001b[32m     27\u001b[39m     true_evals = true_evals.to(device)\n",
      "\u001b[31mNameError\u001b[39m: name 'data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Setup for Training ---\n",
    "\n",
    "# Instantiate the model and move it to the selected device\n",
    "model = ActorCritic().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "learning_rate = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# --- The Training Loop ---\n",
    "\n",
    "num_epochs = 3 # A few epochs are enough for a demonstration\n",
    "# (For real training, you'd run this for many more epochs)\n",
    "\n",
    "print(\"\\n--- Starting Critic Pre-training ---\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    for i, (board_tensors, true_evals) in enumerate(data_loader):\n",
    "        # Move tensors to the correct device\n",
    "        board_tensors = board_tensors.to(device)\n",
    "        true_evals = true_evals.to(device)\n",
    "\n",
    "        if epoch == 0 and i == 0:\n",
    "            print(f\"Sanity Check: Min eval: {torch.min(true_evals)}, Max eval: {torch.max(true_evals)}\")\n",
    "        \n",
    "        # 1. Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward pass: get the model's output\n",
    "        # We only care about the value output for this training step\n",
    "        _ , predicted_evals = model(board_tensors)\n",
    "        \n",
    "        # 3. Calculate the loss\n",
    "        loss = criterion(predicted_evals, true_evals)\n",
    "        \n",
    "        # 4. Backward pass: compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Update the model's weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Print progress every 100 batches\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                  f\"Batch [{i+1}/{len(data_loader)}], \"\n",
    "                  f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"\\n--- Epoch {epoch+1} Finished ---\")\n",
    "    print(f\"Average Loss for Epoch: {avg_loss:.4f}\\n\")\n",
    "\n",
    "print(\"--- Pre-training Finished ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc8d4477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save the pre-trained model weights\n",
    "torch.save(model.state_dict(), 'critic_pretrained_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "932cef4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of possible actions in our mapping: 20480\n",
      "\n",
      "--- Testing the mapping ---\n",
      "The move 'e2e4' corresponds to action index: 3980\n",
      "Action index 3980 on the starting board is the move: e2e4\n",
      "Action index 2030 ('g1g3') on the starting board returns: None\n"
     ]
    }
   ],
   "source": [
    "# Create a list of all possible moves from any square to any square.\n",
    "# This is a simplified but effective mapping. A more complex one handles promotions separately.\n",
    "all_possible_moves = []\n",
    "for from_square in chess.SQUARES:\n",
    "    for to_square in chess.SQUARES:\n",
    "        # This will create moves like 'a1a2', 'a1a3', etc.\n",
    "        all_possible_moves.append(chess.Move(from_square, to_square))\n",
    "        # Handle knight promotions (the most common)\n",
    "        for promotion in [chess.QUEEN, chess.ROOK, chess.BISHOP, chess.KNIGHT]:\n",
    "             all_possible_moves.append(chess.Move(from_square, to_square, promotion=promotion))\n",
    "\n",
    "# Create a dictionary for fast lookups\n",
    "move_to_index = {move: i for i, move in enumerate(all_possible_moves)}\n",
    "\n",
    "# Let's adjust our model's num_actions to match this mapping\n",
    "# This number might be different from 4672, and that's okay.\n",
    "# It's important that the model's output matches our mapping.\n",
    "NUM_ACTIONS = len(all_possible_moves)\n",
    "print(f\"Number of possible actions in our mapping: {NUM_ACTIONS}\")\n",
    "\n",
    "\n",
    "def move_to_action(move: chess.Move) -> int:\n",
    "    \"\"\"\n",
    "    Converts a chess.Move object to its corresponding action index.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return move_to_index[move]\n",
    "    except KeyError:\n",
    "        # This can happen for underpromotions if not handled explicitly,\n",
    "        # but our list is comprehensive.\n",
    "        raise ValueError(f\"Move {move} not found in action mapping.\")\n",
    "\n",
    "def action_to_move(action_index: int, board: chess.Board) -> chess.Move | None:\n",
    "    \"\"\"\n",
    "    Converts an action index back to a chess.Move object.\n",
    "    It's crucial to check if the move is legal in the current board position.\n",
    "    \"\"\"\n",
    "    move = all_possible_moves[action_index]\n",
    "    if move in board.legal_moves:\n",
    "        return move\n",
    "    return None\n",
    "\n",
    "# --- Example Usage ---\n",
    "board = chess.Board()\n",
    "print(\"\\n--- Testing the mapping ---\")\n",
    "\n",
    "# Example 1: Get the action for a legal move\n",
    "move_e2e4 = chess.Move.from_uci(\"e2e4\")\n",
    "action_index = move_to_action(move_e2e4)\n",
    "print(f\"The move 'e2e4' corresponds to action index: {action_index}\")\n",
    "\n",
    "# Example 2: Convert the action index back to a move\n",
    "retrieved_move = action_to_move(action_index, board)\n",
    "print(f\"Action index {action_index} on the starting board is the move: {retrieved_move.uci()}\")\n",
    "\n",
    "# Example 3: Test an illegal move\n",
    "# The action for g1g3 exists, but it's not legal from the start\n",
    "move_g1g3 = chess.Move.from_uci(\"g1g3\")\n",
    "action_g1g3 = move_to_action(move_g1g3)\n",
    "retrieved_illegal_move = action_to_move(action_g1g3, board)\n",
    "print(f\"Action index {action_g1g3} ('g1g3') on the starting board returns: {retrieved_illegal_move}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf1a2404",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NUM_ACTIONS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     75\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Load our pre-trained model\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# NOTE: Make sure NUM_ACTIONS matches the model's output size\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m model = ActorCritic(num_actions=\u001b[43mNUM_ACTIONS\u001b[49m).to(device)\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     81\u001b[39m     \u001b[38;5;66;03m# 2. Load the saved weights into a temporary variable\u001b[39;00m\n\u001b[32m     82\u001b[39m     pretrained_dict = torch.load(\u001b[33m'\u001b[39m\u001b[33mppo_trained_bot.pth\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'NUM_ACTIONS' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "def run_self_play_game(model, device):\n",
    "    \"\"\"\n",
    "    Simulates one full game of self-play.\n",
    "    Returns the collected trajectory data needed for PPO.\n",
    "    \"\"\"\n",
    "    # Lists to store the data for the entire game\n",
    "    trajectory = {\n",
    "        \"states\": [],\n",
    "        \"actions\": [],\n",
    "        \"log_probs\": [],\n",
    "        \"values\": []\n",
    "    }\n",
    "    board = chess.Board()\n",
    "    board_history_fen = [board.fen()] # Store the initial board stat\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad(): # We are not training here, just collecting data\n",
    "        while not board.is_game_over():\n",
    "            # 1. OBSERVE: Get the current state\n",
    "            state_tensor = board_to_tensor(board).unsqueeze(0).to(device)\n",
    "            \n",
    "            # 2. THINK: Get policy and value from the network\n",
    "            policy_logits, value = model(state_tensor)\n",
    "            \n",
    "            # 3. ACT: Sample a legal move from the policy\n",
    "            \n",
    "            # Create a mask for legal moves\n",
    "            legal_move_indices = [move_to_action(move) for move in board.legal_moves]\n",
    "            mask = torch.ones_like(policy_logits) * -1e9 # Mask with a large negative number\n",
    "            mask[0, legal_move_indices] = 0\n",
    "            \n",
    "            # Apply the mask and get probabilities\n",
    "            masked_logits = policy_logits + mask\n",
    "            probs = F.softmax(masked_logits, dim=-1)\n",
    "            \n",
    "            # Use Categorical distribution to sample a move\n",
    "            dist = Categorical(probs)\n",
    "            action_index = dist.sample()\n",
    "            log_prob = dist.log_prob(action_index)\n",
    "            \n",
    "            # 4. RECORD: Store the transition\n",
    "            trajectory[\"states\"].append(state_tensor.squeeze(0))\n",
    "            trajectory[\"actions\"].append(action_index)\n",
    "            trajectory[\"log_probs\"].append(log_prob)\n",
    "            trajectory[\"values\"].append(value)\n",
    "            \n",
    "            # 5. REPEAT: Make the move on the board\n",
    "            move = all_possible_moves[action_index.item()]\n",
    "            board.push(move)\n",
    "\n",
    "            # Store the new board state\n",
    "            board_history_fen.append(board.fen())\n",
    "\n",
    "    # --- Game Finished: Determine the final reward ---\n",
    "    result = board.result()\n",
    "    if result == \"1-0\": # White wins\n",
    "        reward = 1.0\n",
    "    elif result == \"0-1\": # Black wins\n",
    "        reward = -1.0\n",
    "    else: # Draw\n",
    "        reward = 0.0\n",
    "        \n",
    "    # The reward is from the perspective of the player whose turn it is at the end.\n",
    "    # If it's Black's turn at the end (White made the last move), a 1-0 result is good.\n",
    "    # If it's White's turn at the end (Black made the last move), a 1-0 result is bad.\n",
    "    # This logic correctly assigns rewards to the last player.\n",
    "    if board.turn != chess.WHITE:\n",
    "        reward = -reward\n",
    "\n",
    "    return trajectory, reward, board_history_fen\n",
    "\n",
    "# --- Example Usage ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load our pre-trained model\n",
    "# NOTE: Make sure NUM_ACTIONS matches the model's output size\n",
    "model = ActorCritic(num_actions=NUM_ACTIONS).to(device)\n",
    "try:\n",
    "    # 2. Load the saved weights into a temporary variable\n",
    "    pretrained_dict = torch.load('ppo_trained_bot.pth')\n",
    "    \n",
    "    # 3. Get the state dictionary of our new model\n",
    "    model_dict = model.state_dict()\n",
    "    \n",
    "    # 4. Filter out the mismatched weights (the policy_head)\n",
    "    # We create a new dictionary containing only the weights from the saved file\n",
    "    # that exist in the new model AND have the same shape.\n",
    "    filtered_pretrained_dict = {\n",
    "        k: v for k, v in pretrained_dict.items() \n",
    "        if k in model_dict and v.shape == model_dict[k].shape\n",
    "    }\n",
    "    \n",
    "    # 5. Update the new model's dictionary with our filtered weights\n",
    "    model_dict.update(filtered_pretrained_dict)\n",
    "    \n",
    "    # 6. Load the updated dictionary into the model\n",
    "    model.load_state_dict(model_dict)\n",
    "    \n",
    "    print(\"Pre-trained weights for shared body and critic head loaded manually and successfully.\")\n",
    "    num_loaded = len(filtered_pretrained_dict)\n",
    "    print(f\"Loaded {num_loaded} matching weight tensors.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No pre-trained weights found. Using a randomly initialized model.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Running one game of self-play ---\")\n",
    "game_trajectory, final_reward, game_history = run_self_play_game(model, device)\n",
    "print(\"Game finished!\")\n",
    "print(f\"Final reward: {final_reward} in {len(game_history) - 1} moves.\")\n",
    "print(f\"Number of moves in the game: {len(game_trajectory['states'])}\")\n",
    "\n",
    "# Inspect the first step of the trajectory\n",
    "print(\"\\n--- Data from the first move ---\")\n",
    "print(f\"State tensor shape: {game_trajectory['states'][0].shape}\")\n",
    "print(f\"Action index: {game_trajectory['actions'][0].item()}\")\n",
    "print(f\"Log probability: {game_trajectory['log_probs'][0].item():.4f}\")\n",
    "print(f\"Critic's value estimate: {game_trajectory['values'][0].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b6d2ce9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_self_play_game' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# --- Run the game to get the history ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m game_trajectory, final_reward, game_history = \u001b[43mrun_self_play_game\u001b[49m(model, device)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGame finished with reward \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(game_history)-\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m moves.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# --- Visualize the game ---\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'run_self_play_game' is not defined"
     ]
    }
   ],
   "source": [
    "# You might need to install this library\n",
    "# pip install IPython\n",
    "import chess.svg\n",
    "from IPython.display import display, SVG, clear_output\n",
    "import time\n",
    "\n",
    "# --- Run the game to get the history ---\n",
    "game_trajectory, final_reward, game_history = run_self_play_game(model, device)\n",
    "print(f\"Game finished with reward {final_reward} in {len(game_history)-1} moves.\")\n",
    "\n",
    "# --- Visualize the game ---\n",
    "for fen in game_history:\n",
    "    # Create an SVG image of the board\n",
    "    board_svg = chess.svg.board(board=chess.Board(fen), size=350)\n",
    "    \n",
    "    # Display in the notebook\n",
    "    clear_output(wait=True) # Clears the previous board\n",
    "    display(SVG(board_svg))\n",
    "    \n",
    "    # Pause for a moment to see the move\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea3a77d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages_and_returns(rewards, values, gamma=0.99, gae_lambda=0.95):\n",
    "    \"\"\"\n",
    "    Computes advantages and returns for a trajectory using GAE.\n",
    "    \"\"\"\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "    \n",
    "    last_advantage = 0\n",
    "    # The last value is the final reward\n",
    "    last_return = rewards[-1]\n",
    "    \n",
    "    # Iterate backwards from the second to last step\n",
    "    for t in reversed(range(len(rewards) - 1)):\n",
    "        # The value of the next state is needed for the TD error\n",
    "        next_value = values[t+1]\n",
    "        \n",
    "        # Calculate the TD error (delta)\n",
    "        # delta = reward + gamma * V(s_t+1) - V(s_t)\n",
    "        delta = rewards[t] + gamma * next_value - values[t]\n",
    "        \n",
    "        # Calculate the advantage using the GAE formula\n",
    "        # A(s_t) = delta + gamma * lambda * A(s_t+1)\n",
    "        advantages[t] = delta + gamma * gae_lambda * last_advantage\n",
    "        last_advantage = advantages[t]\n",
    "        \n",
    "        # Calculate the return for this step\n",
    "        # R(t) = reward_t + gamma * R(t+1)\n",
    "        returns[t] = rewards[t] + gamma * last_return\n",
    "        last_return = returns[t]\n",
    "        \n",
    "    # The advantage for the final step is just its TD error\n",
    "    advantages[-1] = rewards[-1] - values[-1]\n",
    "    returns[-1] = rewards[-1]\n",
    "    \n",
    "    return advantages, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "442e9e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_play_worker(model_weights, device_str):\n",
    "    \"\"\"\n",
    "    A worker function that plays one game and returns the result.\n",
    "    It reconstructs the model inside the process.\n",
    "    \"\"\"\n",
    "    # Each process needs to have its own model instance\n",
    "    device = torch.device(device_str)\n",
    "    model = ActorCritic(num_actions=NUM_ACTIONS).to(device)\n",
    "    model.load_state_dict(model_weights)\n",
    "    \n",
    "    trajectory, reward, _ = run_self_play_game(model, device)\n",
    "    return trajectory, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1e63725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update(model, optimizer, trajectories, device, epochs=4, clip_epsilon=0.2, value_coeff=0.5, entropy_coeff=0.01):\n",
    "    \"\"\"\n",
    "    Performs the PPO update on a batch of trajectories.\n",
    "    \"\"\"\n",
    "    # 1. Process all trajectories\n",
    "    all_states = []\n",
    "    all_actions = []\n",
    "    all_log_probs = []\n",
    "    all_advantages = []\n",
    "    all_returns = []\n",
    "\n",
    "    for trajectory, final_reward in trajectories:\n",
    "        # Prepare rewards tensor for the entire game\n",
    "        game_rewards = torch.zeros(len(trajectory[\"values\"]))\n",
    "        game_rewards[-1] = final_reward\n",
    "        \n",
    "        # Detach values from graph for advantage calculation\n",
    "        game_values = torch.cat(trajectory[\"values\"]).detach()\n",
    "        \n",
    "        # Calculate advantages and returns\n",
    "        advantages, returns = compute_advantages_and_returns(game_rewards, game_values)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        all_states.extend(trajectory[\"states\"])\n",
    "        all_actions.extend(trajectory[\"actions\"])\n",
    "        all_log_probs.extend(trajectory[\"log_probs\"])\n",
    "        all_advantages.append(advantages)\n",
    "        all_returns.append(returns)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    states_tensor = torch.stack(all_states).to(device)\n",
    "    actions_tensor = torch.stack(all_actions).to(device)\n",
    "    old_log_probs_tensor = torch.stack(all_log_probs).detach().to(device)\n",
    "    advantages_tensor = torch.cat(all_advantages).to(device)\n",
    "    returns_tensor = torch.cat(all_returns).to(device)\n",
    "\n",
    "    total_policy_loss = 0\n",
    "    total_value_loss = 0\n",
    "    total_entropy = 0\n",
    "\n",
    "    # 2. Perform PPO updates for a few epochs\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        # Get new policy logits, values, and action distributions from the model\n",
    "        new_logits, new_values = model(states_tensor)\n",
    "        dist = Categorical(F.softmax(new_logits, dim=-1))\n",
    "        \n",
    "        # Calculate new log probabilities and entropy\n",
    "        new_log_probs = dist.log_prob(actions_tensor.squeeze(-1))\n",
    "        entropy = dist.entropy().mean()\n",
    "        \n",
    "        # --- Calculate the PPO losses ---\n",
    "        # Ratio for the policy loss\n",
    "        ratio = torch.exp(new_log_probs - old_log_probs_tensor.squeeze(-1))\n",
    "        \n",
    "        # Clipped surrogate objective\n",
    "        surr1 = ratio * advantages_tensor\n",
    "        surr2 = torch.clamp(ratio, 1.0 - clip_epsilon, 1.0 + clip_epsilon) * advantages_tensor\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        \n",
    "        # Value loss\n",
    "        value_loss = F.mse_loss(new_values.squeeze(-1), returns_tensor)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = policy_loss + value_coeff * value_loss - entropy_coeff * entropy\n",
    "\n",
    "        # --- ADD THIS: Accumulate the metrics ---\n",
    "        total_policy_loss += policy_loss.item()\n",
    "        total_value_loss += value_loss.item()\n",
    "        total_entropy += entropy.item()\n",
    "        \n",
    "        # --- Update the model ---\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    num_updates = epochs\n",
    "    avg_metrics = {\n",
    "        \"policy_loss\": total_policy_loss / num_updates,\n",
    "        \"value_loss\": total_value_loss / num_updates,\n",
    "        \"entropy\": total_entropy / num_updates\n",
    "    }\n",
    "    return avg_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f1880a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Resuming training from 'ppo_trained_bot.pth'.\n",
      "\n",
      "--- Starting Parallel PPO Training ---\n",
      "--- Starting Update Step 1 ---\n",
      "  - Collecting 20 games using 32 parallel workers...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Setup for Training ---\n",
    "    device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = torch.device(device_str)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Instantiate the main model\n",
    "    model = ActorCritic(num_actions=NUM_ACTIONS).to(device)\n",
    "\n",
    "    # --- Load Weights to Resume Training ---\n",
    "    try:\n",
    "        model.load_state_dict(torch.load('ppo_trained_bot.pth', weights_only=True))\n",
    "        print(\"Resuming training from 'ppo_trained_bot.pth'.\")\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            model.load_state_dict(torch.load('critic_pretrained_weights.pth', weights_only=True), strict=False)\n",
    "            print(\"Starting training from 'critic_pretrained_weights.pth'.\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"No saved weights found. Starting training from scratch.\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    games_per_update = 20 # You can increase this now, e.g., to 50 or 100\n",
    "\n",
    "    # --- Main Training Loop with Parallelization ---\n",
    "    print(\"\\n--- Starting Parallel PPO Training ---\")\n",
    "    for update_step in range(1000): # Run for many steps\n",
    "        print(f\"--- Starting Update Step {update_step + 1} ---\")\n",
    "        \n",
    "        # Get the current model weights to send to workers\n",
    "        model_weights = model.state_dict()\n",
    "        \n",
    "        # Use a Pool to run games in parallel\n",
    "        # mp.cpu_count() uses all available CPU cores\n",
    "        num_workers = mp.cpu_count()\n",
    "        print(f\"  - Collecting {games_per_update} games using {num_workers} parallel workers...\")\n",
    "        with mp.Pool(processes=num_workers) as pool:\n",
    "            # Create a list of arguments for the workers\n",
    "            worker_args = [(model_weights, device_str) for _ in range(games_per_update)]\n",
    "            \n",
    "            # Run the tasks in parallel and collect the results\n",
    "            batch_trajectories = pool.starmap(self_play_worker, worker_args)\n",
    "        \n",
    "        # The rest of the loop is the same\n",
    "        batch_rewards = [r for t, r in batch_trajectories]\n",
    "        \n",
    "        print(\"  - All games finished. Performing PPO update...\")\n",
    "        metrics = ppo_update(model, optimizer, batch_trajectories, device)\n",
    "        avg_reward = sum(batch_rewards) / len(batch_rewards)\n",
    "        \n",
    "        print(\n",
    "            f\"Update: {update_step + 1}, \"\n",
    "            f\"Avg Reward: {avg_reward:.4f}, \"\n",
    "            f\"Value Loss: {metrics['value_loss']:.4f}, \"\n",
    "            f\"Entropy: {metrics['entropy']:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Checkpointing\n",
    "        torch.save(model.state_dict(), 'ppo_trained_bot.pth')\n",
    "\n",
    "    print(\"--- Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a04d9dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bot_move(board, model_to_use):\n",
    "    \"\"\"\n",
    "    Gets a move from the specified model for the given board state.\n",
    "    \n",
    "    Args:\n",
    "        board (chess.Board): The current board state.\n",
    "        model_to_use (torch.nn.Module): The specific model (v1 or v2) to use for inference.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        state_tensor = board_to_tensor(board).unsqueeze(0).to(device)\n",
    "        # Use the model passed as a parameter\n",
    "        policy_logits, _ = model_to_use(state_tensor)\n",
    "\n",
    "        legal_move_indices = [move_to_action(move) for move in board.legal_moves]\n",
    "        mask = torch.ones_like(policy_logits) * -1e9\n",
    "        mask[0, legal_move_indices] = 0\n",
    "        \n",
    "        masked_logits = policy_logits + mask\n",
    "        probs = torch.nn.functional.softmax(masked_logits, dim=-1)\n",
    "        \n",
    "        # Play the best move (argmax) instead of sampling\n",
    "        action_index = torch.argmax(probs).item()\n",
    "        \n",
    "        return all_possible_moves[action_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78c155c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinal Scores after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_games\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m games: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Load both models and run the match\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m device = \u001b[43mtorch\u001b[49m.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# Use CPU for evaluation\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# --- Load Model V2 (The final PPO-trained bot) ---\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading fully trained PPO model (v2)...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import chess\n",
    "\n",
    "def run_match(model_v2, model_v1, num_games=10):\n",
    "    scores = {\"v2_wins\": 0, \"v1_wins\": 0, \"draws\": 0}\n",
    "    \n",
    "    for i in range(num_games):\n",
    "        board = chess.Board()\n",
    "        # Alternate who plays white\n",
    "        players = {chess.WHITE: model_v2, chess.BLACK: model_v1} if i % 2 == 0 else {chess.WHITE: model_v1, chess.BLACK: model_v2}\n",
    "        \n",
    "        while not board.is_game_over():\n",
    "            current_player_model = players[board.turn]\n",
    "            move = get_bot_move(board, current_player_model) # Assuming you have a get_bot_move function\n",
    "            board.push(move)\n",
    "            \n",
    "        result = board.result()\n",
    "        # ... (add logic here to parse the result and update scores) ...\n",
    "        print(f\"Game {i+1} finished. Result: {result}\")\n",
    "        \n",
    "    print(f\"Final Scores after {num_games} games: {scores}\")\n",
    "\n",
    "# Load both models and run the match\n",
    "device = torch.device(\"cpu\") # Use CPU for evaluation\n",
    "\n",
    "# --- Load Model V2 (The final PPO-trained bot) ---\n",
    "print(\"Loading fully trained PPO model (v2)...\")\n",
    "model_v2 = ActorCritic(num_actions=NUM_ACTIONS).to(device)\n",
    "# These weights were saved from the final model, so they will match perfectly.\n",
    "model_v2.load_state_dict(torch.load('ppo_trained_bot.pth', weights_only=True))\n",
    "model_v2.eval() # Set to evaluation mode\n",
    "print(\"Model v2 loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- Load Model V1 (The original critic-only bot) ---\n",
    "print(\"\\nLoading original critic-only model (v1)...\")\n",
    "model_v1 = ActorCritic(num_actions=NUM_ACTIONS).to(device)\n",
    "try:\n",
    "    # Use the manual loading method we developed to handle the size mismatch\n",
    "    # in the policy head, loading only the critic and shared body weights.\n",
    "    pretrained_dict = torch.load('critic_pretrained_weights.pth', weights_only=True)\n",
    "    model_dict = model_v1.state_dict()\n",
    "    \n",
    "    filtered_dict = {\n",
    "        k: v for k, v in pretrained_dict.items() \n",
    "        if k in model_dict and v.shape == model_dict[k].shape\n",
    "    }\n",
    "    \n",
    "    model_dict.update(filtered_dict)\n",
    "    model_v1.load_state_dict(model_dict)\n",
    "    \n",
    "    print(\"Model v1 loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Could not find critic_pretrained_weights.pth. Model v1 will be random.\")\n",
    "model_v1.eval() # Set to evaluation mode\n",
    "run_match(model_v2, model_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc60c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained weights for shared body and critic head loaded manually and successfully.\n",
      "Loaded 27 matching weight tensors.\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rama Ranuh\\AppData\\Local\\Temp\\ipykernel_33256\\3286563805.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_dict = torch.load('critic_pretrained_weights.pth')\n",
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:03] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:03] \"GET /img/chesspieces/wikipedia/bR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:03] \"GET /img/chesspieces/wikipedia/bQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:03] \"GET /img/chesspieces/wikipedia/wP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:03] \"GET /img/chesspieces/wikipedia/bK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:03] \"GET /img/chesspieces/wikipedia/bN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:03] \"GET /img/chesspieces/wikipedia/bB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:03] \"GET /img/chesspieces/wikipedia/bP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:03] \"GET /img/chesspieces/wikipedia/wR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:03] \"GET /img/chesspieces/wikipedia/wN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:03] \"GET /img/chesspieces/wikipedia/wB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:03] \"GET /img/chesspieces/wikipedia/wQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:03] \"GET /img/chesspieces/wikipedia/wK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:04] \"GET /img/chesspieces/wikipedia/wP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:04] \"POST /move HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:04] \"GET /img/chesspieces/wikipedia/bK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:04] \"GET /img/chesspieces/wikipedia/bN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:04] \"GET /img/chesspieces/wikipedia/wP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:04] \"GET /img/chesspieces/wikipedia/bP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:04] \"GET /img/chesspieces/wikipedia/bR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:04] \"GET /img/chesspieces/wikipedia/bQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:04] \"GET /img/chesspieces/wikipedia/bB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:04] \"GET /img/chesspieces/wikipedia/wR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:04] \"GET /img/chesspieces/wikipedia/wN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:04] \"GET /img/chesspieces/wikipedia/wB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:04] \"GET /img/chesspieces/wikipedia/wQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:04] \"GET /img/chesspieces/wikipedia/wK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:05] \"GET /img/chesspieces/wikipedia/bP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:05] \"GET /img/chesspieces/wikipedia/wP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:05] \"GET /img/chesspieces/wikipedia/bQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:05] \"GET /img/chesspieces/wikipedia/bR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:05] \"GET /img/chesspieces/wikipedia/bN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:05] \"GET /img/chesspieces/wikipedia/bK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:05] \"GET /img/chesspieces/wikipedia/wB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:05] \"GET /img/chesspieces/wikipedia/bB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:05] \"GET /img/chesspieces/wikipedia/wR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:05] \"GET /img/chesspieces/wikipedia/wN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:05] \"GET /img/chesspieces/wikipedia/wQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:05] \"GET /img/chesspieces/wikipedia/wK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:05] \"GET /img/chesspieces/wikipedia/wP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"POST /move HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/bP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/bN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/bQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/bR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/wP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/bB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/bK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/wR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/wN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/wB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/wQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/wK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/bP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/bN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/bB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/bR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/bK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/bQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/wR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/wP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/wN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/wB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/wQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:06] \"GET /img/chesspieces/wikipedia/wK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/wP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"POST /move HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/bB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/bN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/bR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/wP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/bP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/bK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/bQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/wR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/wN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/wB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/wQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/wK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/bB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/bN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/bR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/bQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/bK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/bP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/wP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/wR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/wN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/wB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/wQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:07] \"GET /img/chesspieces/wikipedia/wK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/wP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"POST /move HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/bB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/bP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/bR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/bK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/bN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/bQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/wP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/wR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/wN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/wB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/wQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/wK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/bQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/bR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/bN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/bK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/bP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/bB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/wP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/wR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/wN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/wB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/wQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:08] \"GET /img/chesspieces/wikipedia/wK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"POST /move HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"POST /move HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/bR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:09] \"GET /img/chesspieces/wikipedia/wK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:10] \"GET /img/chesspieces/wikipedia/bQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:11] \"GET /img/chesspieces/wikipedia/bP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:11] \"GET /img/chesspieces/wikipedia/bR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:11] \"GET /img/chesspieces/wikipedia/bK.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:11] \"GET /img/chesspieces/wikipedia/wP.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:11] \"GET /img/chesspieces/wikipedia/bB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:11] \"GET /img/chesspieces/wikipedia/bN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:11] \"GET /img/chesspieces/wikipedia/wR.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:11] \"GET /img/chesspieces/wikipedia/wN.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:11] \"GET /img/chesspieces/wikipedia/wQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:11] \"GET /img/chesspieces/wikipedia/wB.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:11] \"GET /img/chesspieces/wikipedia/bQ.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [24/Aug/2025 18:47:11] \"GET /img/chesspieces/wikipedia/wK.png HTTP/1.1\" 404 -\n"
     ]
    }
   ],
   "source": [
    "import chess\n",
    "import torch\n",
    "from flask import Flask, render_template, request, jsonify, session\n",
    "from flask_session import Session\n",
    "\n",
    "# (Place your ActorCritic model, board_to_tensor, and move mapping functions here)\n",
    "# ... (or import them from another file)\n",
    "\n",
    "# --- Initialize the Bot ---\n",
    "device = torch.device(\"cpu\") # Use CPU for inference on a web server\n",
    "model = ActorCritic(num_actions=NUM_ACTIONS).to(device)\n",
    "try:\n",
    "    # 2. Load the saved weights into a temporary variable\n",
    "    pretrained_dict = torch.load('critic_pretrained_weights.pth')\n",
    "    \n",
    "    # 3. Get the state dictionary of our new model\n",
    "    model_dict = model.state_dict()\n",
    "    \n",
    "    # 4. Filter out the mismatched weights (the policy_head)\n",
    "    # We create a new dictionary containing only the weights from the saved file\n",
    "    # that exist in the new model AND have the same shape.\n",
    "    filtered_pretrained_dict = {\n",
    "        k: v for k, v in pretrained_dict.items() \n",
    "        if k in model_dict and v.shape == model_dict[k].shape\n",
    "    }\n",
    "    \n",
    "    # 5. Update the new model's dictionary with our filtered weights\n",
    "    model_dict.update(filtered_pretrained_dict)\n",
    "    \n",
    "    # 6. Load the updated dictionary into the model\n",
    "    model.load_state_dict(model_dict)\n",
    "    \n",
    "    print(\"Pre-trained weights for shared body and critic head loaded manually and successfully.\")\n",
    "    num_loaded = len(filtered_pretrained_dict)\n",
    "    print(f\"Loaded {num_loaded} matching weight tensors.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No pre-trained weights found. Using a randomly initialized model.\")\n",
    "model.eval()\n",
    "\n",
    "# --- Flask App Setup ---\n",
    "app = Flask(__name__)\n",
    "app.config[\"SESSION_PERMANENT\"] = False\n",
    "app.config[\"SESSION_TYPE\"] = \"filesystem\"\n",
    "Session(app)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    \"\"\"\n",
    "    Start a new game and render the board.\n",
    "    \"\"\"\n",
    "    if \"board\" not in session:\n",
    "        board = chess.Board()\n",
    "        session[\"board\"] = board.fen()\n",
    "    \n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "@app.route(\"/move\", methods=[\"POST\"])\n",
    "def move():\n",
    "    \"\"\"\n",
    "    Handle a player's move and respond with the bot's move.\n",
    "    \"\"\"\n",
    "    board = chess.Board(session[\"board\"])\n",
    "    \n",
    "    # Get the player's move from the request\n",
    "    player_move_uci = request.json.get(\"move\")\n",
    "    move = chess.Move.from_uci(player_move_uci)\n",
    "    \n",
    "    # Make the player's move\n",
    "    if move in board.legal_moves:\n",
    "        board.push(move)\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Illegal move\"}), 400\n",
    "    \n",
    "    # Check if the game is over\n",
    "    if board.is_game_over():\n",
    "        session[\"board\"] = board.fen()\n",
    "        return jsonify({\"fen\": board.fen(), \"game_over\": True, \"result\": board.result()})\n",
    "    \n",
    "    # Get and make the bot's move\n",
    "    bot_move = get_bot_move(board)\n",
    "    board.push(bot_move)\n",
    "    \n",
    "    # Store the new board state\n",
    "    session[\"board\"] = board.fen()\n",
    "    \n",
    "    # Check if the game is over after the bot's move\n",
    "    game_over = board.is_game_over()\n",
    "    result = board.result() if game_over else None\n",
    "    \n",
    "    return jsonify({\"fen\": board.fen(), \"game_over\": game_over, \"result\": result})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, use_reloader=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
